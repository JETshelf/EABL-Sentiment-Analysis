{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Driven Brand Mastery: EABL's Social Analytics\n",
    "\n",
    "\n",
    "## Business Overview\n",
    "media for brand monitoring to enhance brand perception and customer engagement.\n",
    "East African Breweries Limited (EABL) is a leading Fast-Moving Consumer Goods (FMCG) company operating in the beverage industry.\n",
    "EABL's market footprint extends across East Africa, with a significant presence in countries such as Kenya, Uganda, Tanzania, Rwanda, and South Sudan.\n",
    "With a diverse portfolio of brands such as Tusker Lager, Pilsner Lager, WhiteCap Lager,  Johnnie Walker, Smirnoff, Gilbey's Gin, Richot Brandy, Bond 7 Whiskey and Baileys Irish Cream.\n",
    "As part of its strategic approach, EABL is keen on leveraging social \n",
    "\n",
    "##  Problem Statement\n",
    "EABL recognizes the need to closely monitor and understand the sentiments expressed on social media platforms regarding its brands, products, and marketing initiatives. The company aims to address the following challenges:\n",
    "\n",
    "#### 1. Brand Perception and Engagement: \n",
    "\n",
    "The current state of public sentiment towards EABL on social media is not well-understood, and the company lacks insights to proactively manage and improve its brand perception.\n",
    "\n",
    "#### 2. Marketing Campaign Effectiveness: \n",
    "\n",
    "EABL needs to evaluate the impact of its marketing campaigns and promotions on social media to optimize strategies and refine future campaigns.\n",
    "\n",
    "#### 3. Customer Feedback Analysis: \n",
    "\n",
    "There is a lack of systematic analysis of customer feedback related to product quality, packaging, pricing, and overall satisfaction. EABL aims to utilize this feedback for continuous product improvement and innovation.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "### Main Objectives\n",
    "\n",
    "#### Enhance Brand Perception and Customer Engagement:\n",
    "\n",
    "The main objective of our project is to monitor and analyze social media conversations to understand public sentiment towards EABL brands.\n",
    "Identifying areas for improvement and take proactive measures to enhance overall brand perception.\n",
    "\n",
    "### Specific Objectives\n",
    "\n",
    "#### Brand Sentiment Analysis:\n",
    "\n",
    "Analyze sentiments expressed by consumers regarding EABL products, marketing campaigns, and brand image.\n",
    "\n",
    "Categorize sentiments as positive, negative, or neutral to derive actionable insights.\n",
    "\n",
    "#### Campaign Effectiveness:\n",
    "\n",
    "Evaluate the effectiveness of marketing campaigns and promotions through public reactions on social media.\n",
    "\n",
    "Measure key performance indicators (KPIs) such as engagement rates, reach, and sentiment shift during and after campaigns.\n",
    "\n",
    "#### Customer Feedback Analysis:\n",
    "\n",
    "Capture and analyze customer feedback on specific EABL products.\n",
    "\n",
    "Extract insights on taste, packaging, pricing, and overall satisfaction to guide product development and enhancement.\n",
    "\n",
    "######  Success Criteria\n",
    "\n",
    "The success of this project will be measured by:\n",
    "\n",
    "1. A notable improvement in overall brand sentiment scores over a defined period.\n",
    "\n",
    "2. Positive shifts in sentiment during and after key marketing campaigns.\n",
    "\n",
    "3. Tangible product improvements based on customer feedback.\n",
    "\n",
    "4. Enhanced online visibility and engagement compared to competitors.\n",
    "\n",
    "Stakeholders\n",
    "Marketing Team:\n",
    "\n",
    "Benefit from insights to optimize marketing strategies and refine campaigns.\n",
    "Product Development Team:\n",
    "\n",
    "Utilize feedback for continuous product improvement and innovation.\n",
    "Executive Leadership:\n",
    "\n",
    "Receive regular reports on brand health and public perception for strategic decision-making. Objectives\n",
    "\n",
    "\n",
    "## Stakeholders\n",
    "\n",
    "#### 1. Marketing Team:\n",
    "\n",
    "Benefit from insights to optimize marketing strategies and refine campaigns.\n",
    "\n",
    "#### 2. Product Development Team:\n",
    "\n",
    "Utilize feedback for continuous product improvement and innovation.\n",
    "\n",
    "#### 3. Executive Leadership:\n",
    "\n",
    "Receive regular reports on brand health and public perception for strategic decision-making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "#### 1. Brand Mentions:\n",
    "\n",
    "Social media posts, comments, and mentions related to EABL and its brands.\n",
    "\n",
    "Data sources: Twitter, Facebook, Instagram (Likes, shares, comments, EABL relevant hashtags.)\n",
    "\n",
    "\n",
    "#### Product Feedback:\n",
    "\n",
    "Comments and discussions regarding specific EABL products.\n",
    "\n",
    "#### Data sources: \n",
    "\n",
    "Social media platforms, review sites, online forums.\n",
    "\n",
    "Data on taste, packaging, pricing, and overall satisfaction.\n",
    "\n",
    "#### Campaign Reactions:\n",
    "\n",
    "Public reactions to EABL's marketing campaigns and promotions.\n",
    "\n",
    "Data sources: Social media platforms.\n",
    "\n",
    "#### Event Participation:\n",
    "\n",
    "Social media mentions related to EABL-sponsored events and promotions.\n",
    "\n",
    "#### Customer Service Interactions:\n",
    "\n",
    "Social media interactions with EABL's official customer service handles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FinalMentions.CSV'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e0d151350a06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reading the datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mEabl_mentions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FinalMentions.CSV\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mEabl_hashtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FinalHashtags.CSV\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mEabl_brands\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eablProducts.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mGoogle_news\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GoogleNewsData.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FinalMentions.CSV'"
     ]
    }
   ],
   "source": [
    "# reading the datasets\n",
    "Eabl_mentions = pd.read_csv(\"FinalMentions.CSV\")\n",
    "Eabl_hashtags = pd.read_csv(\"FinalHashtags.CSV\")\n",
    "Eabl_brands = pd.read_csv(\"eablProducts.csv\")\n",
    "Google_news = pd.read_excel(\"GoogleNewsData.xlsx\")\n",
    "Chrome_Gin = pd.read_csv(\"ChromeGin.CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MERGING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining  the DataFrames vertically to form one df\n",
    "combined_df = pd.concat([Eabl_mentions, Eabl_hashtags, Eabl_brands, Chrome_Gin], ignore_index=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"CombinedFinal.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our Final Combined data has {combined_df.shape[0]} rows & {combined_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the Unnamed: 0 column\n",
    "combined_df = combined_df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in each column\n",
    "missing_values = combined_df.isna().sum()\n",
    "\n",
    "for column, count in missing_values.items():\n",
    "    if count > 0:\n",
    "        print(f\"The {column} column has {count} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of missing values in each column\n",
    "missing_percentage = combined_df.isna().mean() * 100\n",
    "\n",
    "missing_percentage = missing_percentage[missing_percentage > 0]\n",
    "\n",
    "# A DataFrame with columns and percentage of missing values\n",
    "missing_table = pd.DataFrame({\n",
    "    'Columns': missing_percentage.index,\n",
    "    '% of Missing Values': missing_percentage.values\n",
    "})\n",
    "print(\"Percentage of Missing Values\")\n",
    "missing_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the 'cleaned_text', 'sentiment','stats columns\n",
    "combined_df = combined_df.dropna(subset=['cleaned_text','sentiment','stats'])\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2. Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for dupliacated rows in the df\n",
    "duplicated_rows = combined_df.duplicated().sum()\n",
    "print(f'The DataFrame has {duplicated_rows} duplicated rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentages1 = (combined_df.isnull().sum() / len(combined_df)) * 100\n",
    "\n",
    "# Print the result\n",
    "print(\"Percentage of missing values in each column:\")\n",
    "print(missing_percentages1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all the duplicates\n",
    "combined_df = combined_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = combined_df.duplicated().sum()\n",
    "print(f'The DataFrame has {duplicated_rows} duplicated rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  3. Correcting Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  4. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# columns to check for outliers\n",
    "columns_to_check = ['comments', 'retweets', 'quotes', 'likes']\n",
    "\n",
    "# subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "# Flatten the axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot box plots for each column\n",
    "for i, column in enumerate(columns_to_check):\n",
    "    sns.boxplot(x=combined_df[column], ax=axes[i])\n",
    "    axes[i].set_title(f'Box Plot - {column}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Removing Uncessary Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Removing any unnecessary characters, symbols, or special characters\n",
    "    cleaned_text = ' '.join(e for e in str(text).split() if (e.isalnum() or e.isspace()))\n",
    "    # Converting all the text to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    # Removing extra whitespaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "# Applying the cleaning function to the 'cleaned_text' column\n",
    "combined_df['cleaned_text'] = combined_df['cleaned_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Length of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating  the number of words in the 'cleaned_text' column\n",
    "combined_df['text_length_words'] = combined_df['cleaned_text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "combined_df[['cleaned_text', 'text_length_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Maximum number of words is {combined_df['text_length_words'].max()} & The Minimum number of words is {combined_df['text_length_words'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Datetime Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'stats' column to datetime format\n",
    "combined_df['stats'] = pd.to_datetime(combined_df['stats'], format='%b %d, %Y · %I:%M %p UTC', errors='coerce')\n",
    "\n",
    "# Extracting datetime features\n",
    "combined_df['day'] = combined_df['stats'].dt.day\n",
    "combined_df['month'] = combined_df['stats'].dt.month\n",
    "combined_df['year'] = combined_df['stats'].dt.year\n",
    "combined_df['hour'] = combined_df['stats'].dt.hour\n",
    "combined_df['day_of_week'] = combined_df['stats'].dt.dayofweek\n",
    "\n",
    "combined_df[['stats', 'day', 'month', 'year', 'hour','day_of_week']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'day_of_week' to text\n",
    "day_of_week_mapping = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "combined_df['day_of_week_text'] = combined_df['day_of_week'].map(day_of_week_mapping)\n",
    "\n",
    "# Converting 'month' to text\n",
    "month_mapping = {1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June', 7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n",
    "combined_df['month_text'] = combined_df['month'].map(month_mapping)\n",
    "\n",
    "combined_df[['stats', 'day', 'month', 'year', 'hour','day_of_week_text', 'month_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Total Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the total number of engagement in each post\n",
    "combined_df['total_engagement'] = combined_df['comments'] + combined_df['retweets'] + combined_df['quotes'] + combined_df['likes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the keywords\n",
    "# keywords = ['Chrome', 'Oktoberfest', 'Gilbeys']\n",
    "\n",
    "# # Create columns for each keyword\n",
    "# for keyword in keywords:\n",
    "#     combined_df[f'mentions_{keyword.lower()}'] = combined_df['cleaned_text'].str.contains(keyword, case=False)\n",
    "\n",
    "# # Convert the boolean values to 1 for True and 0 for False\n",
    "# for keyword in keywords:\n",
    "#     combined_df[f'mentions_{keyword.lower()}'] = combined_df[f'mentions_{keyword.lower()}'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Total Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to classify weekdays and weekends\n",
    "def classify_weekday_weekend(day_of_week):\n",
    "    if day_of_week < 5:  # Monday to Friday (0 to 4)\n",
    "        return 'Weekday'\n",
    "    else:  # Saturday and Sunday (5 and 6)\n",
    "        return 'Weekend'\n",
    "\n",
    "# Apply the function to create a new 'weekday_or_weekend' column\n",
    "combined_df['weekday_or_weekend'] = combined_df['day_of_week'].apply(classify_weekday_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns\n",
    "combined_df = combined_df.drop(['stats','day_of_week','month','mentions_keyword'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "combined_df = combined_df.rename(columns={\n",
    "    'cleaned_text': 'text',\n",
    "    'text_length_words': 'length',\n",
    "    'day': 'day(month)',\n",
    "    'day_of_week_text': 'day(week)',\n",
    "    'month_text': 'month',\n",
    "    'weekday_or_weekend': 'weekday/end'\n",
    "})\n",
    "\n",
    "# Display the DataFrame with the renamed columns\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the summary of the combined data_set before embarking on the EDA\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
